{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFq7IFxoJIJs",
        "outputId": "b119d30b-3b21-4b0d-b672-1e65503cf28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = \"/content/drive/MyDrive/Datasets for CMPE 255/species.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "6ayzck79a4it"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**:\n",
        "  \n",
        "  The Data Preparation phase is where the raw data is transformed and readied for modeling. It's often said that data preparation can take up a significant portion of a data science project because the quality of data directly impacts the performance of models.\n",
        "\n",
        "For this dataset, the Data Preparation phase can be detailed as follows:\n",
        "\n",
        "1. **Data Cleaning**:\n",
        "\n",
        "  **Handling Missing Values**:\n",
        "  The Iris dataset is generally clean, but in real-world scenarios, datasets might have missing values. Strategies to handle them include imputation (filling missing values based on certain criteria) or deletion (removing records with missing values).\n",
        "\n",
        "  **Removing Duplicates**:\n",
        "  Ensure that the dataset doesn't have any duplicate entries which could bias the model.\n",
        "\n",
        "2. **Data Transformation**:\n",
        "  \n",
        "  **Feature Engineering**:\n",
        "Based on the insights from the data understanding phase, new features might be derived or existing ones might be transformed to better capture the underlying patterns. For instance, the ratio of petal length to width might be a useful feature.\n",
        "\n",
        "  **Normalization/Standardization**:\n",
        "Some machine learning algorithms, like SVM or k-NN, are sensitive to the scale of features. Features might need to be normalized (scaled to [0,1]) or standardized (scaled to have mean 0 and variance 1).\n",
        "\n",
        "3. **Data Reduction**:\n",
        "  \n",
        "  **Feature Selection**:\n",
        "If there were many features, redundant or irrelevant features might need to be removed. This isn't typically necessary for the Iris dataset due to its simplicity, but in larger datasets, techniques like Recursive Feature Elimination or feature importance from tree-based algorithms might be employed.\n",
        "\n",
        "4. **Data Splitting**:\n",
        "  \n",
        "  **Training and Testing Sets**:\n",
        "The dataset should be split into a training set to train the model and a testing set to evaluate its performance. A common split ratio is 80% for training and 20% for testing.\n",
        "\n",
        "  **Stratified Split**:\n",
        "Given that the Iris dataset has three equally represented classes, it's essential to ensure that the train-test split maintains this class distribution. Stratified sampling ensures that each split has the same proportion of classes as the whole dataset.\n",
        "  \n",
        "5. **Data Encoding**:\n",
        "  \n",
        "  **Categorical Variable Encoding**:\n",
        "Machine learning models require numerical input, so categorical variables (like the species in the Iris dataset) need to be encoded. Common techniques include one-hot encoding or label encoding. Since species is the target variable and it's categorical, it would need to be encoded to numeric labels.\n",
        "\n",
        "6. **Challenges and Considerations**:\n",
        "  \n",
        "  **Overfitting**:\n",
        "If too many features are used or if the training data is overly processed, the model might perform exceptionally well on the training data but poorly on new, unseen data. This phenomenon is called overfitting.\n",
        "  \n",
        "  **Data Leakage**:\n",
        "Ensure that any transformation or imputation is learned only from the training data and then applied to the testing data. Using information from the test set during the data preparation phase can lead to overly optimistic performance estimates.\n",
        "\n",
        "In summary, the Data Preparation phase ensures that the dataset is in the best possible format and quality for modeling. Properly prepared data can lead to better model performance and more accurate insights."
      ],
      "metadata": {
        "id": "Ox6lLpusbHJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgO4gmpxcMjT",
        "outputId": "2ac36a11-15c8-4829-95bf-f33d081ad155"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sepal length    0\n",
              "sepal width     0\n",
              "petal length    0\n",
              "petal width     0\n",
              "class           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Splitting the dataset into training and testing sets is a crucial step in the data preparation process. By doing so, you can train the model on one subset and test it on another, unseen subset, to evaluate its performance.`\n",
        "\n"
      ],
      "metadata": {
        "id": "yGZ6eoRkcmoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = df.drop('class', axis=1)  # Features (sepal and petal measurements)\n",
        "y = df['class']  # Target variable (species)\n",
        "\n",
        "# Split the dataset: 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "WFibVUNbc3d8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`With this code:`\n",
        "\n",
        "`X_train and y_train are the features and target variable for the training set, respectively.`\n",
        "\n",
        "`X_test and y_test are the features and target variable for the testing set, respectively.`\n",
        "\n",
        "`test_size=0.2 indicates that 20% of the data will be used for testing.`\n",
        "\n",
        "`random_state=42 ensures reproducibility. Any integer value can be used, and it ensures that the data is split in the same way every time the code is run.`\n",
        "\n",
        "`After splitting, you can proceed with training models using the training set (X_train and y_train) and then evaluate their performance using the testing set (X_test and y_test)`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CMiFqK6RdJ-0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M426K03ddzXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}